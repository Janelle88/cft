}
}
if (length(dim(v)[3]) == 0 | is.na(dim(v)[3])) {
var2 = .orient(v, fun = fun)
b1 = raster::raster(var2)
raster::crs(b1) = g$proj
raster::extent(b1) = g$e
}
else {
var2 = array(0, dim = c(dim(v)[2], dim(v)[1],
dim(v)[3]))
for (j in 1:dim(v)[3]) {
var2[, , j] = .orient(v[, , j], fun = fun)
}
b1 = raster::brick(var2)
raster::crs(b1) = g$proj
raster::extent(b1) = g$e
}
b1[b1 > 1e+05] = NA
b[[i]] = b1
}
names(b) = names
keys = unique(names(b))
b = sapply(keys, function(name) {
stack(b[grep(name, names(b))])
})
for (i in 1:length(b)) {
names(b[[i]]) = unique(date.names)
}
}
>>>>>>> 211da197c901d9f19965c9f34f12cb68d0378b54
#' Install and load packages, what is the convention in R?
# Installer
loadInstall <- function(packages){
new_packages = packages[!(packages %in% installed.packages()[, "Package"])]
if (length(new_packages) > 0)
install.packages(new_packages, dependencies = TRUE)
}
# cran packages (sometimes requires some user input in linux)
packages <- c('aws.s3', 'devtools', 'leaflet', 'leaflet.opacity', 'lubridate', 'lwgeom',
'ncdf4', 'progress', 'rasterVis', 'readtext', 'reprex', 'rgdal', 'rgeos',
'sf', 'tidync', 'tidyverse')
loadInstall(packages)
# not-cran packages
devtools::install_github("mikejohnson51/climateR", force=TRUE)
#' This will get all data for any one set of parameters, stack them, and save to s3
library(aws.s3)
library(climateR)
library(readtext)
source('R/getScenario.R')
source('R/getScenario2.R')
source('R/makeNC.R')
main <- function(location = "Death Valley", startDate = "1950-01-01", endDate = "2099-12-31",
method = "maca", param = "prcp", model = "CCSM4", scenario = "rcp45",
timeRes = "daily", year_range = 5, plotSample = FALSE){
'
location = "Wind Cave"
method = "maca"
param = "tmin"
model = "CCSM4"
scenario = "rcp45"
startDate = "1950-01-01"
endDate =  "2099-12-31"
timeRes = "daily"
year_range = 4
plotsample = FALSE
'
# Subsetting by National Parks
if (!exists("parks")){
parks <- rgdal::readOGR("data/shapefiles/nps_boundary.shp", verbose = FALSE)
}
# Get the perimeter and signal that it's working
AOI <- parks[grepl(tolower(location), tolower(parks$UNIT_NAME)),]  # <----------------There will cometimes be two (e.g. "X park" and "X preserve") so fix that
loc <- as.character(AOI$UNIT_NAME)
print(paste0("Retrieving requested data for ", loc))
# Query data and save to disk and return directory
folder <- getScenario(AOI, method = method, param = param, model = model,
scenario = scenario, startDate = startDate, endDate = endDate,
timeRes = timeRes, year_range = year_range, plotsample = FALSE)
# folder <- getScenario2(AOI, method = method, param = param, model = model,
#                        scenario = scenario, startDate = startDate, endDate = endDate,
#                        timeRes = timeRes, year_range = year_range, plotsample = FALSE)
# Merge into one file - might need to sort first when downloaded in parallel
files <- file.path(folder, "????????_????????.nc")  # <------------------------------- Silly syntax, but this works rather quickly
call <- paste0("ncrcat --hst ", files, " ", folder, ".nc")
system(call)
outfile <- paste0(folder, '.nc')
# Push file to s3
creds <- readtext("~/.aws/credentials.txt")[[2]]
creds = strsplit(creds, "\n")
key = substr(creds[[1]][2], 14, 33)
skey = substr(creds[[1]][3], 14, 53)
region = substr(creds[[1]][4], 11, 19)
# Sign in to the account
Sys.setenv("AWS_ACCESS_KEY_ID" = key,
"AWS_SECRET_ACCESS_KEY" = skey,
"AWS_DEFAULT_REGION" = region)
# Put the file in the bucket
bucket_name <- "cstdata-test"
location_folder <- gsub(" ", "_", tolower(as.character(AOI$UNIT_NAME)))
object <- file.path(location_folder, basename(outfile))
put_folder(location_folder, bucket_name)
put_object(file = outfile, object = object, bucket = bucket_name)
# Example retrieval from bucket
# aws.s3::save_object(object, bucket_name, file = "/home/travis/Desktop/test.nc")
}
main()
library(RNetCDF)
library(glue)
source('R/parameters.R')
# Sample aoi
parks <- rgdal::readOGR("data/shapefiles/nps_boundary.shp")
aoi <- parks[grepl("Death Valley", parks$UNIT_NAME),]
# There are two sets of V2 MACA, aggregated and split into 5 year chunks
base = "http://thredds.northwestknowledge.net:8080/thredds/dodsC/agg_macav2metdata"
urls = c("http://thredds.northwestknowledge.net:8080/thredds/dodsC/agg_macav2metdata_tasmin_CCSM4_r6i1p1_historical_1950_2005_CONUS_daily.nc?air_temperature[0:1:20453][442:1:446][509:1:515]",
"http://thredds.northwestknowledge.net:8080/thredds/dodsC/agg_macav2metdata_tasmin_CCSM4_r6i1p1_rcp45_2006_2099_CONUS_daily.nc?air_temperature[0:1:34332][442:1:446][509:1:515]")
# Get the extent coordinates
lonmin <- aoi@bbox[1,1]
lonmax <- aoi@bbox[1,2]
latmin <- aoi@bbox[2,1]
latmax <- aoi@bbox[2,2]
# Now use these from the lat/lons of the full grid to get index positions
lonmindiffs <- abs(lons - lonmin)
lonmaxdiffs <- abs(lons - lonmax)
latmindiffs <- abs(lats - latmin)
latmaxdiffs <- abs(lats - latmax)
lon1 <- match(lonmindiffs[lonmindiffs == min(lonmindiffs)], lonmindiffs)
lon2 <- match(lonmaxdiffs[lonmaxdiffs == min(lonmaxdiffs)], lonmaxdiffs)
lat1 <- match(latmindiffs[latmindiffs == min(latmindiffs)], latmindiffs)
lat2 <- match(latmaxdiffs[latmaxdiffs == min(latmaxdiffs)], latmaxdiffs)
# Now we can build the urls, starting with history of course
hurls <- c()
for (p in seq(length(params))){
for (m in seq(length(models))){
for (s in seq(length(scenarios))){
attachment <- paste(c(base, params[p], models[m], ensembles[m], scenarios[s],
"HISTORICAL", "1950_2005_CONUS_daily.nc?"), collapse = "_")
var <- variables[p]
query <- paste0(var, glue("[{0}:{1}:{ntime_hist}][{lat1}:{1}:{lat2}][{lon1}:{1}:{lon2}]"))
url <- paste0(attachment, query)
hurls <- append(hurls, url)
}
}
}
# And the "future"
furls = c()
for (p in seq(length(params))){
for (m in seq(length(models))){
for (s in seq(length(scenarios))){
attachment <- paste(c(base, params[p], models[m], ensembles[m], scenarios[s],
"2006_2099_CONUS_daily.nc?"), collapse = "_")
var <- variables[p]
query <- paste0(var, glue("[{0}:{1}:{ntime_future}][{lat1}:{1}:{lat2}][{lon1}:{1}:{lon2}]"))
url <- paste0(attachment, query)
furls <- append(furls, url)
}
}
}
# I want these together
urls <- unlist(lapply(1:360, function(i) c(hurls[i], furls[i])))
# Now, let's get each in pairs using an even number of cores
`%dopar%` <- foreach::`%dopar%`
no_cores <- parallel::detectCores()
urls
# I want these together
urls <- unlist(lapply(1:360, function(i) c(hurls[i], furls[i])))
urls
# Now, let's get each in pairs using an even number of cores
`%dopar%` <- foreach::`%dopar%`
no_cores <- parallel::detectCores()
if (no_cores %% 2 == 0){
ncores <- no_cores - 2
}else{
ncores <- no_cores - 1
}
# Now I want them in groups of ncores
urls2 <- split(urls, ceiling(seq_along(ncores)))
urls2
# Now I want them in groups of ncores
hurls2 <- split(hurls, ceiling(seq_along(ncores)))
hurls2
seq_along(ncores)
ncores
ceiling(seq_along(ncores))
# Now I want them in groups of ncores
hurls2 <- split(hurls, ceiling(seq_along(hurls)/6))
hurls2
ncores <- parallel::detectCores() - 1
doParallel::registerDoParallel(ncores)
# Now I want them in groups of ncores
hurls2 <- split(hurls, ceiling(seq_along(hurls)/ncores))
hurls2
furls2 <- split(furls, ceiling(seq_along(furls)/ncores))
var.get.nc(
i
i
urls[i]
var <- urls[i][regexpr('?', urls[i]), regexpr('[', urls[i])]
regexpr('?', urls[i]
# This is the most basic extraction
system.time({
data1 = RNetCDF::open.nc(url) %>% RNetCDF::var.get.nc(variables[1])
data2 = RNetCDF::open.nc(url) %>% RNetCDF::var.get.nc(variables[1], unpack = T)
data = list(data1, data2)
})
# This is how he parallelizes
system.time({
`%dopar%` <- foreach::`%dopar%`
no_cores <- parallel::detectCores() - 1
doParallel::registerDoParallel(no_cores)
data = foreach::foreach(i = 1:length(urls)) %dopar% {
RNetCDF::open.nc(urls[i]) %>% RNetCDF::var.get.nc(params[i], unpack = T)
}
})
regexpr('?', urls[i])
regexpr('[', urls[i])
urls[i])
urls[i]
urls[i]
regexpr('?', urls[i]), regexpr('[', urls[i])
var <- urls[i][regexpr('?', urls[i]), regexpr('[', urls[i])]
urls[i]
urls[i][4:7]
urls[i][c(4, 7)]
urls[i]
stringr::str_locate(urls[i], "?")
stringr::str_locate(urls[[i]], "?")
urls[[1]]
var <- urls[[i]][regexpr('?', urls[[i]]), regexpr('[', urls[[i]])]
str_locate(urls[[1]], "?")
stringr::str_locate(urls[[1]], "?")
stringr::str_locate(as.character(urls[[1]]), "?")
var <- urls[[i]][gregexpr('?', urls[[i]]), gregexpr('[', urls[[i]])]
urls[i]
gregexpr('[', urls[i])
gregexpr('?', urls[[i]]
)
gregexpr(pattern = '?', urls[[i]])
stringi::str_locate('?', urls[[i]])
stringi::stri_locate('?', urls[[i]])
stringi::stri_locate('?', urls[[i]], fixed = TRUE)
stringi::stri_locate('?', urls[[i]], fixed = TRUE)
stringi::stri_locate('?', urls[i], fixed = TRUE)
lapply(strsplit(urls[i], ''), function(x) which(x == '?'))
urls[i]
urls[i][142]
data = foreach::foreach(i = 1:length(urls)) %dopar% {
v1 = lapply(strsplit(urls[i], ''), function(x) which(x == "?"))
var <- urls[[i]][gregexpr(pattern = '?', urls[[i]]), gregexpr('[', urls[i])]
RNetCDF::open.nc(urls[i]) %>% RNetCDF::var.get.nc(var, unpack = T)
}
v1 = lapply(strsplit(urls[i], ''), function(x) which(x == "?"))
v2 = lapply(strsplit(urls[i], ''), function(x) which(x == "["))
v2
v2 = lapply(strsplit(urls[i], ''), function(x) which(x == "["))[1]
v2
v2 = lapply(strsplit(urls[i], ''), function(x) which(x == "[")[1])
v2
var <- urls[[i]][v1:v2]
v1:v2
v1
v2
var <- urls[[i]][as.integer(v1):as.integer(v2)]
var
var <- substr(urls[[i]], as.integer(v1), as.integer(v2))
var
var <- substr(urls[[i]], v1, v2)
var
v1 = lapply(strsplit(urls[i], ''), function(x) which(x == "?")) + 1
v2 = lapply(strsplit(urls[i], ''), function(x) which(x == "[")[1]) - 1
var <- substr(urls[[i]], v1, v2)
v1
v2 = lapply(strsplit(urls[i], ''), function(x) which(x == "[")[1]) - 1
var <- substr(urls[[i]], as.integer(v1) + 1, as.integer(v2) + 1)
var
var <- substr(urls[[i]], as.integer(v1) + 1, as.integer(v2) - 1)
var
RNetCDF::open.nc(urls[i]) %>% RNetCDF::var.get.nc(var, unpack = T)
data1 = RNetCDF::open.nc(url) %>% RNetCDF::var.get.nc(variables[1])
library(RNetCDF)
library(glue)
source('R/parameters.R')
# Sample aoi
parks <- rgdal::readOGR("data/shapefiles/nps_boundary.shp")
aoi <- parks[grepl("Death Valley", parks$UNIT_NAME),]
# There are two sets of V2 MACA, aggregated and split into 5 year chunks
base = "http://thredds.northwestknowledge.net:8080/thredds/dodsC/agg_macav2metdata"
urls = c("http://thredds.northwestknowledge.net:8080/thredds/dodsC/agg_macav2metdata_tasmin_CCSM4_r6i1p1_historical_1950_2005_CONUS_daily.nc?air_temperature[0:1:20453][442:1:446][509:1:515]",
"http://thredds.northwestknowledge.net:8080/thredds/dodsC/agg_macav2metdata_tasmin_CCSM4_r6i1p1_rcp45_2006_2099_CONUS_daily.nc?air_temperature[0:1:34332][442:1:446][509:1:515]")
# Get the extent coordinates
lonmin <- aoi@bbox[1,1]
lonmax <- aoi@bbox[1,2]
latmin <- aoi@bbox[2,1]
latmax <- aoi@bbox[2,2]
# Now use these from the lat/lons of the full grid to get index positions
lonmindiffs <- abs(lons - lonmin)
lonmaxdiffs <- abs(lons - lonmax)
latmindiffs <- abs(lats - latmin)
latmaxdiffs <- abs(lats - latmax)
lon1 <- match(lonmindiffs[lonmindiffs == min(lonmindiffs)], lonmindiffs)
lon2 <- match(lonmaxdiffs[lonmaxdiffs == min(lonmaxdiffs)], lonmaxdiffs)
lat1 <- match(latmindiffs[latmindiffs == min(latmindiffs)], latmindiffs)
lat2 <- match(latmaxdiffs[latmaxdiffs == min(latmaxdiffs)], latmaxdiffs)
# Now we can build the urls, starting with history of course
hurls <- c()
for (p in seq(length(params))){
for (m in seq(length(models))){
for (s in seq(length(scenarios))){
attachment <- paste(c(base, params[p], models[m], ensembles[m], scenarios[s],
"HISTORICAL", "1950_2005_CONUS_daily.nc?"), collapse = "_")
var <- variables[p]
query <- paste0(var, glue("[{0}:{1}:{ntime_hist}][{lat1}:{1}:{lat2}][{lon1}:{1}:{lon2}]"))
url <- paste0(attachment, query)
hurls <- append(hurls, url)
}
}
}
# And the "future"
furls = c()
for (p in seq(length(params))){
for (m in seq(length(models))){
for (s in seq(length(scenarios))){
attachment <- paste(c(base, params[p], models[m], ensembles[m], scenarios[s],
"2006_2099_CONUS_daily.nc?"), collapse = "_")
var <- variables[p]
query <- paste0(var, glue("[{0}:{1}:{ntime_future}][{lat1}:{1}:{lat2}][{lon1}:{1}:{lon2}]"))
url <- paste0(attachment, query)
furls <- append(furls, url)
}
}
}
# Now, let's get each in pairs using an even number of cores
`%dopar%` <- foreach::`%dopar%`
ncores <- parallel::detectCores() - 1
doParallel::registerDoParallel(ncores)
# Now I want them in groups of ncores
hurls2 <- split(hurls, ceiling(seq_along(hurls)/ncores))
furls2 <- split(furls, ceiling(seq_along(furls)/ncores))
urls[i], ''
urls
i
v1 = lapply(strsplit(hurls[i], ''), function(x) which(x == "?"))
v2 = lapply(strsplit(hurls[i], ''), function(x) which(x == "[")[1])
v1
v2
v2 = lapply(strsplit(hurls[i], ''), function(x) which(x == "[")[1])
var <- substr(hurls[[i]], as.integer(v1) + 1, as.integer(v2) - 1)
var
RNetCDF::open.nc(hurls[i]) %>% RNetCDF::var.get.nc(var, unpack = T)
#' This will get all data for any one set of parameters, stack them, and save to s3
library(aws.s3)
library(climateR)
library(readtext)
source('R/getScenario.R')
source('R/getScenario2.R')
source('R/makeNC.R')
RNetCDF::open.nc(hurls[i]) %>% RNetCDF::var.get.nc(var, unpack = T)
hurls[i]
var, unpack = T
var
urls
RNetCDF::open.nc(urls[i]) %>% RNetCDF::var.get.nc(var, unpack = T)
hurls
# Okay, now we need a function that can take
for (chunk in seq(length(hurls))){
print(chunk)
}
# Okay, now we need a function that can take
for (chunk in seq(length(hurls2))){
print(chunk)
}
# Okay, now we need a function that can take
for (chunk in seq(length(hurls2))){
print(chunk)
data <- foreach::foreach(i = 1:length(urls)) %dopar% {
v1 <- lapply(strsplit(hurls[i], ''), function(x) which(x == "?"))
v2 <- lapply(strsplit(hurls[i], ''), function(x) which(x == "[")[1])
var <- substr(hurls[[i]], as.integer(v1) + 1, as.integer(v2) - 1)
RNetCDF::open.nc(urls[i]) %>% RNetCDF::var.get.nc(var, unpack = T)
}
}
RNetCDF::open.nc(hurls[i]) %>% RNetCDF::var.get.nc(var, unpack = T)
# Okay, now we need a function that can take
for (chunk in seq(length(hurls2))){
print(chunk)
data <- foreach::foreach(i = 1:length(urls)) %dopar% {
v1 <- lapply(strsplit(hurls[i], ''), function(x) which(x == "?"))
v2 <- lapply(strsplit(hurls[i], ''), function(x) which(x == "[")[1])
var <- substr(hurls[[i]], as.integer(v1) + 1, as.integer(v2) - 1)
RNetCDF::open.nc(hurls[i]) %>% RNetCDF::var.get.nc(var, unpack = T)
}
}
hurls
i
i = 10
RNetCDF::open.nc(hurls[i]) %>% RNetCDF::var.get.nc(var, unpack = T)
urls
# Now we can build the urls, starting with history of course
hurls <- c()
for (p in seq(length(params))){
for (m in seq(length(models))){
for (s in seq(length(scenarios))){
attachment <- paste(c(base, params[p], models[m], ensembles[m],
"HISTORICAL", "1950_2005_CONUS_daily.nc?"), collapse = "_")
var <- variables[p]
query <- paste0(var, glue("[{0}:{1}:{ntime_hist}][{lat1}:{1}:{lat2}][{lon1}:{1}:{lon2}]"))
url <- paste0(attachment, query)
hurls <- append(hurls, url)
}
}
}
# And the "future"
furls = c()
for (p in seq(length(params))){
for (m in seq(length(models))){
for (s in seq(length(scenarios))){
attachment <- paste(c(base, params[p], models[m], ensembles[m], scenarios[s],
"2006_2099_CONUS_daily.nc?"), collapse = "_")
var <- variables[p]
query <- paste0(var, glue("[{0}:{1}:{ntime_future}][{lat1}:{1}:{lat2}][{lon1}:{1}:{lon2}]"))
url <- paste0(attachment, query)
furls <- append(furls, url)
}
}
}
# Now I want them in groups of ncores
hurls2 <- split(hurls, ceiling(seq_along(hurls)/ncores))
furls2 <- split(furls, ceiling(seq_along(furls)/ncores))
RNetCDF::open.nc(hurls2[i]) %>% RNetCDF::var.get.nc(var, unpack = T)
hurls2[i]
RNetCDF::open.nc(hurls[i]) %>% RNetCDF::var.get.nc(var, unpack = T)
hurls[i]
urls
# Now we can build the urls, starting with history of course
hurls <- c()
for (p in seq(length(params))){
for (m in seq(length(models))){
for (s in seq(length(scenarios))){
attachment <- paste(c(base, params[p], models[m], ensembles[m],
"historical", "1950_2005_CONUS_daily.nc?"), collapse = "_")
var <- variables[p]
query <- paste0(var, glue("[{0}:{1}:{ntime_hist}][{lat1}:{1}:{lat2}][{lon1}:{1}:{lon2}]"))
url <- paste0(attachment, query)
hurls <- append(hurls, url)
}
}
}
# And the "future"
furls = c()
for (p in seq(length(params))){
for (m in seq(length(models))){
for (s in seq(length(scenarios))){
attachment <- paste(c(base, params[p], models[m], ensembles[m], scenarios[s],
"2006_2099_CONUS_daily.nc?"), collapse = "_")
var <- variables[p]
query <- paste0(var, glue("[{0}:{1}:{ntime_future}][{lat1}:{1}:{lat2}][{lon1}:{1}:{lon2}]"))
url <- paste0(attachment, query)
furls <- append(furls, url)
}
}
}
RNetCDF::open.nc(hurls[i]) %>% RNetCDF::var.get.nc(var, unpack = T)
var
v1 <- lapply(strsplit(hurls[i], ''), function(x) which(x == "?"))
v2 <- lapply(strsplit(hurls[i], ''), function(x) which(x == "[")[1])
var <- substr(hurls[[i]], as.integer(v1) + 1, as.integer(v2) - 1)
RNetCDF::open.nc(hurls[i]) %>% RNetCDF::var.get.nc(var, unpack = T)
print(chunk)
hurls[chunk]
hurls2[chunk]
1:length(hurls2[chunk])
ength(hurls2[chunk]))
hurls2[chunk]
length(hurls2[chunk])
length(hurls2[[chunk]])
# Okay, now we need a function that can take the results of these and save to file in s3 (check if these need to be transposed).
for (chunk in seq(length(hurls2))){
print(chunk)
data <- foreach::foreach(i = 1:length(hurls2[[chunk]])) %dopar% {
v1 <- lapply(strsplit(hurls2[[chunk]][i], ''), function(x) which(x == "?"))
v2 <- lapply(strsplit(hurls2[[chunk]][i], ''), function(x) which(x == "[")[1])
var <- substr(hurls2[[chunk]][[i]], as.integer(v1) + 1, as.integer(v2) - 1)
print(var)
# single <- RNetCDF::open.nc(hurls2[[chunk]][i]) %>% RNetCDF::var.get.nc(var, unpack = T)
# Possibly, transpose matrix
# Create grid, then raster
# makeNC
}
}
# Okay, now we need a function that can take the results of these and save to file in s3 (check if these need to be transposed).
for (chunk in seq(length(hurls2))){
print(chunk)
foreach::foreach(i = 1:length(hurls2[[chunk]])) %dopar% {
v1 <- lapply(strsplit(hurls2[[chunk]][i], ''), function(x) which(x == "?"))
v2 <- lapply(strsplit(hurls2[[chunk]][i], ''), function(x) which(x == "[")[1])
var <- substr(hurls2[[chunk]][[i]], as.integer(v1) + 1, as.integer(v2) - 1)
print(var)
# single <- RNetCDF::open.nc(hurls2[[chunk]][i]) %>% RNetCDF::var.get.nc(var, unpack = T)
# Possibly, transpose matrix
# Create grid, then raster
# makeNC
}
}
getwd
getwd()
ls
getwd()
getwd()
dir
dir()
setwd("R")
