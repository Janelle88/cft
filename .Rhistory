# List of urls for a sample set of parameters
urls <- read.table('data/macav2metdata_urls.txt')
urls <- lapply(urls, as.character)[[1]]
urls <- urls[grep('historical', urls) & grep(scenario, urls)]
grep('historical', urls)
urls <- urls[grep('historical', urls) | grep(scenario, urls)]
grep(scenario, urls)
grep('historical', urls)
urls <- urls[(grep('historical', urls) | grep(scenario, urls))]
scenario
urls <- urls[which(grep('historical', urls) | grep(scenario, urls))]
which(grep('historical', urls) | grep(scenario, urls))
urls <- urls[grep('historical', urls)]
urls
urls <- urls[c(grep('historical', urls), grep(scenario, urls))]
# List of urls for a sample set of parameters
urls <- read.table('data/macav2metdata_urls.txt')
urls <- lapply(urls, as.character)[[1]]
urls <- urls[c(grep('historical', urls), grep(scenario, urls))]
urls
urls = lapply(urls, function(x) gsub('fileServer', 'dodsC', x))
urls
# List of urls for a sample set of parameters
urls <- read.table('data/macav2metdata_urls.txt')
urls <- lapply(urls, as.character)[[1]]
urls <- urls[c(grep('historical', urls), grep(scenario, urls))]
urls = lapply(urls, function(x) gsub('fileServer', 'dodsC', x))[[1]]
urls
urls <- read.table('data/macav2metdata_urls.txt')
urls <- lapply(urls, as.character)[[1]]
urls <- urls[c(grep('historical', urls), grep(scenario, urls))]
urls = lapply(urls, function(x) gsub('fileServer', 'dodsC', x))
urls
# List of urls for a sample set of parameters
urls <- read.table('data/macav2metdata_urls.txt')
urls
urls <- lapply(urls, as.character)[[1]]
urls
# List of urls for a sample set of parameters
urls <- vector(read.table('data/macav2metdata_urls.txt'))
# List of urls for a sample set of parameters
urls <- read.table('data/macav2metdata_urls.txt')
urls <- urls[c(grep('historical', urls), grep(scenario, urls))]
urls
# List of urls for a sample set of parameters
urls <- read.table('data/macav2metdata_urls.txt')
urls <- lapply(urls, as.character)[[1]]
urls <- urls[c(grep('historical', urls), grep(scenario, urls))]
urls
urls <- lapply(urls, function(x) gsub('fileServer', 'dodsC', x))
urls
urls <- unlist(lapply(urls, function(x) gsub('fileServer', 'dodsC', x)))
urls
ncdf4::::nc_open(urls[1])
ncdf4::nc_open(urls[1])
ncdf4::nc_open(urls[[1]])
raster::raster(urls[1])
# List of urls for a sample set of parameters
urls <- read.table('data/macav2metdata_urls.txt')
urls <- lapply(urls, as.character)[[1]]
urls <- urls[c(grep('historical', urls), grep(scenario, urls))]
urls
# Can I get a pointer to a multifile dataset (a la xarray.open_mfdataset())
test <- nc_open(urls[1])
urls <- unlist(lapply(urls, function(x) gsub('fileServer', 'dodsC', x)))
# Can I get a pointer to a multifile dataset (a la xarray.open_mfdataset())
test <- nc_open(urls[1])
library(ncdf4)
library(RNetCDF)
# Can I get a pointer to a multifile dataset (a la xarray.open_mfdataset())
test <- RNetCDF::open.nc(urls[1])
test
# Can I get a pointer to a multifile dataset (a la xarray.open_mfdataset())
test <- RNetCDF::open.nc(urls)
test
test
# Can I get a pointer to a multifile dataset (a la xarray.open_mfdataset())
test <- RNetCDF::open.nc(urls, write = TRUE)
# Can I get a pointer to a multifile dataset (a la xarray.open_mfdataset())
test <- RNetCDF::open.nc(urls, write = TRUE)
test
library(tidync)
test <- tidync(urls[1])
devtools::install_github("bocinsky/thredds")
library(thredds)
library(tidyverse)
# cran packages (sometimes requires some user input in linux)
packages <- c('aws.s3', 'devtools', 'leaflet', 'leaflet.opacity', 'lubridate', 'lwgeom',
'ncdf4', 'progress', 'rasterVis', 'readtext', 'reprex', 'rgdal', 'rgeos',
'sf', 'tidync', 'tidyverse')
loadInstall(packages)
library(tidyverse)
library(thredds)
urls
# Can I get a pointer to a multifile dataset (a la xarray.open_mfdataset())
# test <- RNetCDF::open.nc(urls)
# test <- tidync(urls[1])
files <- tds_list_datasets(thredds_url = 'http://thredds.northwestknowledge.net:8080/thredds/dodsC/MACAV2/CCSM')
# Can I get a pointer to a multifile dataset (a la xarray.open_mfdataset())
# test <- RNetCDF::open.nc(urls)
# test <- tidync(urls[1])
files <- tds_list_datasets(thredds_url = 'http://thredds.northwestknowledge.net:8080/thredds/')
files
# Can I get a pointer to a multifile dataset (a la xarray.open_mfdataset())
# test <- RNetCDF::open.nc(urls)
# test <- tidync(urls[1])
datasets <- tds_list_datasets(thredds_url = 'http://thredds.northwestknowledge.net:8080/thredds/')
datasets
datasets
us <- datasets[1,]$path
us
usgs <- datasets[1,]$path
usgs <- datasets[1,]$path
usgs
services <- tds_list_services(usgs)
usgs
services <- tds_list_services(usgs)
services
usgs
services <- tds_list_services("http://thredds.northwestknowledge.net:8080/thredds//thredds/nw.csc.is.catalog")
services <- tds_list_services("http://thredds.northwestknowledge.net:8080/thredds//thredds/nw.csc.is.catalog.html")
# Experimenting with this THREDDS package
datasets <- tds_list_datasets(thredds_url = 'http://thredds.northwestknowledge.net:8080/thredds/')
usgs <- datasets[1,]$path
usgs
datasets
# Seems to struggle displaying service, don't think we need this though
url <- urls[0]
url
urls
# Seems to struggle displaying service, don't think we need this though
url <- urls[1]
url
tds_ncss_list_vars(ncss_url = url)
tds_ncss_list_vars
ncss_url
tds_ncss_list_vars(ncss_url = url)
ncss_url
ncss_url = url
out_file = 'data/thredds_test.nc'
out_file = 'data/thredds_test.nc'
bbox
bbox = bb
bbox
vars = NULL
ncss_args = NULL
overwrite = TRUE
base_url <- ncss_url %>%
stringr::str_remove("([^/]+$)") %>%
stringr::str_remove("(\\/+$)")
base_url
ncss_url
vars
if(is.null(vars)){
vars <- tds_ncss_list_vars(ncss_url)
}
tds_ncss_list_vars
tds_ncss_list_vars(ncss_url)
vars <- specific_humidity
vars <- 'specific_humidity''
vars <- 'specific_humidity'
if(is.null(vars)){
vars <- tds_ncss_list_vars(ncss_url)
}
query <- list(var = vars %>% stringr::str_c(collapse = ","),
north = bbox[["ymax"]],
west = bbox[["xmin"]],
east = bbox[["xmax"]],
south = bbox[["ymin"]])
query
bbox
ncss_args
stringr::str_c("./",basename(base_url),".nc")
base_url
base_url
out_file
out_file <- stringr::str_c("./",basename(base_url),".nc")
out_file
out_file = 'data/thredds_test.nc'
bbox = c()
bbox[['ymax']] = bb[2,2]
bb
bbox[['yin']] = bb[2,1]
bbox[['xmin']] = bb[1,1]
bbox[['ymin']] = bb[2,1]
bbax[['xmax']]
bbox[['xmax']]
bbox[['xmax']] = bb[1, 2]
bbox
bb
bbox = bbox[names(bbox) != 'yin']
bbox = bbox[names(bbox) != 'yin']
bbox
bb
vars
query <- list(var = vars %>% stringr::str_c(collapse = ","),
north = bbox[["ymax"]],
west = bbox[["xmin"]],
east = bbox[["xmax"]],
south = bbox[["ymin"]])
query
dir.create(dirname(out_file),
recursive = TRUE,
showWarnings = FALSE)
dirname(out_file)
httr::GET(base_url,
query = query,
httr::write_disk(out_file,
overwrite = overwrite),
...)
tds_ncss_list_vars(ncss_url = url, var = 'specific_humidity')
tds_ncss_list_vars(ncss_url = url, vars = 'specific_humidity')
bbox
bb
bbox = c()
bbox[['ymax']] = bb[2,2]
bbox[['xmin']] = bb[1,1]
bbox[['ymin']] = bb[2,1]
bbox[['xmax']] = bb[1,2]
bbox
bb
bbox = c()
bbox[['xmin']] = bb[1,1]
bbox[['xmax']] = bb[1,2]
bbox[['ymin']] = bb[2,1]
bbox[['ymax']] = bb[2,2]
tds_ncss_download(ncss_url = urls[1], vars = 'specific_humidity', bbox = bbox, overwrite = TRUE, out_file = "data/thredds.nc")
tds_ncss_download(ncss_url = urls[1], vars = 'specific_humidity', out_file = "data/thredds.nc")
# Experimenting with this THREDDS package
bbox = c()
bbox[['xmin']] = bb[1,1] + 360
bbox[['xmax']] = bb[1,2] + 360
bbox[['ymin']] = bb[2,1]
bbox[['ymax']] = bb[2,2]
tds_ncss_download(ncss_url = urls[1], vars = 'specific_humidity', bbox = bbox, overwrite = TRUE, out_file = "data/thredds.nc")
urls[1]
tds_ncss_download(ncss_url = urls[1], vars = c('specific_humidity'), bbox = bbox, overwrite = TRUE, out_file = "data/thredds.nc")
urls[1]
loca_url <-  "https://cida.usgs.gov/thredds/ncss/loca_future/dataset.html"
CCSM4 <- tds_ncss_download(ncss_url = loca_ncss,
out_file = paste0(tempdir(),"/CCSM4.nc"),
bbox = sf::st_bbox(c(xmin = -116.5, xmax = -115, ymin = 44.5, ymax = 45)),
vars = c("tasmax_CCSM4_r6i1p1_rcp45","tasmax_CCSM4_r6i1p1_rcp85"),
ncss_args = list(temporal = "all"))
loca_ncss <-  "https://cida.usgs.gov/thredds/ncss/loca_future/dataset.html"
CCSM4 <- tds_ncss_download(ncss_url = loca_ncss,
out_file = paste0(tempdir(),"/CCSM4.nc"),
bbox = sf::st_bbox(c(xmin = -116.5, xmax = -115, ymin = 44.5, ymax = 45)),
vars = c("tasmax_CCSM4_r6i1p1_rcp45","tasmax_CCSM4_r6i1p1_rcp85"),
ncss_args = list(temporal = "all"))
CCSM4 <- tds_ncss_download(ncss_url = loca_ncss,
out_file ="data/CCSM4.nc"),
bbox = sf::st_bbox(c(xmin = -116.5, xmax = -115, ymin = 44.5, ymax = 45)),
vars = c("tasmax_CCSM4_r6i1p1_rcp45","tasmax_CCSM4_r6i1p1_rcp85"),
ncss_args = list(temporal = "all"))
CCSM4 <- tds_ncss_download(ncss_url = loca_ncss,
out_file ="data/CCSM4.nc",
bbox = sf::st_bbox(c(xmin = -116.5, xmax = -115, ymin = 44.5, ymax = 45)),
vars = c("tasmax_CCSM4_r6i1p1_rcp45","tasmax_CCSM4_r6i1p1_rcp85"),
ncss_args = list(temporal = "all"))
library(climateR)
urls[1]
model = "CCSM4"
# Get the perimeter and signal that it's working
AOI <- parks[grepl(tolower(location), tolower(parks$UNIT_NAME)),]  # <----------------There will cometimes be two (e.g. "X park" and "X preserve") so fix that
param
scenario
startDate
endDate
timeRes
id = "maca"
base = "http://thredds.northwestknowledge.net:8080/thredds/dodsC/agg_macav2metdata_"
base
if (!timeRes %in% c("daily", "monthly")) {
stop("timeRes must be monthly or daily")
}
d = define.dates(startDate, endDate, baseDate = "1950-01-01",
splitDate = "2006-01-01")
define.dates
define
define.dates <- function (startDate, endDate, baseDate = NULL, splitDate = NULL)
{
bd = build.date(startDate, endDate)
dates = seq.Date(as.Date(bd$startDate), as.Date(bd$endDate),
1)
if (!is.null(splitDate)) {
pre = dates[as.Date(dates) < as.Date(splitDate)]
post = dates[as.Date(dates) >= as.Date(splitDate)]
dates = list(pre, post)
}
else {
dates = list(dates)
}
for (i in 1:length(dates)) {
dates[[i]] = data.frame(date = dates[[i]], year = as.numeric(format(dates[[i]],
"%Y")), julien = as.numeric(format(dates[[i]], "%j")))
}
if (!is.null(baseDate)) {
base = c(baseDate, splitDate)
for (i in 1:length(dates)) {
dates[[i]]$date.index = as.numeric(dates[[i]]$date -
as.Date(base[i]))
dates[[i]]$month.index = .numberOfMonths(dates[[i]]$date,
base[i])
}
}
for (i in 1:length(dates)) {
dates[[i]]$string = format(dates[[i]]$date, format = "%Y%m%d")
}
if (length(dates) == 2) {
dates = rbind(dates[[1]], dates[[2]])
}
if (class(dates) == "list") {
dates = dates[[1]]
}
return(dates)
}
d = define.dates(startDate, endDate, baseDate = "1950-01-01",
splitDate = "2006-01-01")
AOI
param
id = "maca"
base = "http://thredds.northwestknowledge.net:8080/thredds/dodsC/agg_macav2metdata_"
if (!timeRes %in% c("daily", "monthly")) {
stop("timeRes must be monthly or daily")
}
d = climateR:::define.dates(startDate, endDate, baseDate = "1950-01-01",
splitDate = "2006-01-01")
v = climateR:::define.versions(dates = d, scenario = scenario, future.call = paste0("2006_2099_CONUS_",
timeRes, ".nc?"), historic.call = paste0("1950_2005_CONUS_",
timeRes, ".nc?"), timeRes = timeRes)
p = define.param(param, service = "maca")
p = climateR:::define.param(param, service = "maca")
k = climateR:::define.config(dataset = "maca", model = model, ensemble = NA)
tmp = expand.grid(min.date = v$min.date, model = k, call = p$call,
stringsAsFactors = FALSE)
d
define.dates(
v = climateR:::define.versions(dates = d, scenario = scenario, future.call = paste0("2006_2099_CONUS_",
timeRes, ".nc?"), historic.call = paste0("1950_2005_CONUS_",
timeRes, ".nc?"), timeRes = timeRes)
v
p
k = climateR:::define.config(dataset = "maca", model = model, ensemble = NA)
k
p
fin
fin = merge(v, tmp, "min.date") %>% merge(p, "call") %>%
merge(model_meta$maca, "model")
fin
if (timeRes == "monthly") {
name.date = format(d$date, "%Y-%m")
}
else {
name.date = d$date
}
fin = merge(v, tmp, "min.date") %>% merge(p, "call") %>%
merge(model_meta$maca, "model")
if (timeRes == "monthly") {
name.date = format(d$date, "%Y-%m")
}
else {
name.date = d$date
}
if (timeRes == "monthly") {
name.date = format(d$date, "%Y-%m")
}
if (timeRes == "monthly") {
name.date = format(d$date, "%Y-%m")
}
else {
name.date = d$date
}
name.date = d$date
name.date
g = define.grid3(AOI, source = id)
g
g = define.grid3(AOI, source = id)
g = climateR:::define.grid3(AOI, source = id)
g
urls = paste0(base, fin$call, "_", fin$model, "_", fin$ensemble,
"_", fin$ver, "_", fin$calls, fin$call2, fin$time.index,
g$lat.call, g$lon.call)
urls
pp = paste0(fin$model, "_", fin$common.name)
s = fast.download(urls, params = fin$call2, names = pp,
g, name.date, dataset = id, fun = "r")
s = climateR:::fast.download(urls, params = fin$call2, names = pp,
g, name.date, dataset = id, fun = "r")
urls
s
urls
params
param
fin$call2
fin$call2
params = fin$call2
names = pp
pp
g
name.date
dataset = id
fun = "r"
no_data = NA
scale_factor = 1
stopifnot(identical(length(urls), NROW(params)))
urls
identical(length(urls), NROW(params))
NROW(params)
params
`%dopar%` <- foreach::`%dopar%`
no_cores <- parallel::detectCores() - 1
no_cores
doParallel::registerDoParallel(no_cores)
g$type
var = foreach::foreach(i = 1:length(urls)) %dopar% {
RNetCDF::open.nc(urls[i]) %>% RNetCDF::var.get.nc(params[i],
unpack = T)
}
library(dplyr)
var = foreach::foreach(i = 1:length(urls)) %dopar% {
RNetCDF::open.nc(urls[i]) %>% RNetCDF::var.get.nc(params[i],
unpack = T)
}
library(dplyr)
%>%
%dopar%
var = foreach::foreach(i = 1:length(urls)) %dopar% {
RNetCDF::open.nc(urls[i]) %>% RNetCDF::var.get.nc(params[i],
unpack = T)
}
library(magrittr)
var = foreach::foreach(i = 1:length(urls)) %dopar% {
RNetCDF::open.nc(urls[i]) %>% RNetCDF::var.get.nc(params[i],
unpack = T)
}
stopifnot(identical(length(urls), NROW(params)))
`%dopar%` <- foreach::`%dopar%`
no_cores <- parallel::detectCores() - 1
doParallel::registerDoParallel(no_cores)
if (g$type == "point") {
var = foreach::foreach(i = 1:length(urls), .combine = "c") %dopar%
{
RNetCDF::open.nc(urls[i]) %>% RNetCDF::var.get.nc(params[i],
unpack = T)
}
}
g$type
var = foreach::foreach(i = 1:length(urls)) %dopar% {
RNetCDF::open.nc(urls[i]) %>% RNetCDF::var.get.nc(params[i],
unpack = T)
}
var = foreach::foreach(i = 1:length(urls)) %dopar% {
RNetCDF::open.nc(urls[i]) %>% RNetCDF::var.get.nc(params[i],
unpack = T)
}
library(dplyr)
var = foreach::foreach(i = 1:length(urls)) %dopar% {
RNetCDF::open.nc(urls[i]) %>% RNetCDF::var.get.nc(params[i],
unpack = T)
}
var = foreach::foreach(i = 1:length(urls)) %dopar% {
RNetCDF::open.nc(urls[i]) %>% RNetCDF::var.get.nc(params[i],
unpack = T)
}
library(magrittr)
foreach::foreach(i = 1:length(urls))
RNetCDF::open.nc(urls[i]) %>% RNetCDF::var.get.nc(params[i],
unpack = T)
i = 1
RNetCDF::open.nc(urls[i]) %>% RNetCDF::var.get.nc(params[i],
unpack = T)
if (g$type == "grid") {
b = list()
for (i in 1:length(var)) {
v = var[[i]] * scale_factor
if (!is.na(no_data)) {
if (no_data > 0) {
v[v > no_data] = NA
}
else {
v[v < no_data] = NA
}
}
if (length(dim(v)[3]) == 0 | is.na(dim(v)[3])) {
var2 = .orient(v, fun = fun)
b1 = raster::raster(var2)
raster::crs(b1) = g$proj
raster::extent(b1) = g$e
}
else {
var2 = array(0, dim = c(dim(v)[2], dim(v)[1],
dim(v)[3]))
for (j in 1:dim(v)[3]) {
var2[, , j] = .orient(v[, , j], fun = fun)
}
b1 = raster::brick(var2)
raster::crs(b1) = g$proj
raster::extent(b1) = g$e
}
b1[b1 > 1e+05] = NA
b[[i]] = b1
}
names(b) = names
keys = unique(names(b))
b = sapply(keys, function(name) {
stack(b[grep(name, names(b))])
})
for (i in 1:length(b)) {
names(b[[i]]) = unique(date.names)
}
}
