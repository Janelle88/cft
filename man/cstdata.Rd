% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cstdata.R
\name{cstdata}
\alias{cstdata}
\title{Climate Scenario Toolkit Data}
\usage{
cstdata(parkname = "Acadia National Park", start_year = 1950,
  end_year = 2099, store_locally = TRUE, local_dir = "cstdata",
  store_remotely = TRUE, aws_config_dir = "~/.aws", verbose = FALSE)
}
\arguments{
\item{parkname}{The name of the U.S. National Park for which to download data
, e.g., "Yellowstone National Park". (character)}

\item{start_year}{The first year of the desired period. (integer)}

\item{end_year}{The last year of the desired period. (integer)}

\item{store_locally}{If `TRUE` this function will store the results in a
local directory as NetCDF files. This options may be set to `FALSE` to save
disk space, but the `store_remotely` option must be set to `TRUE` in this
case. (logical)}

\item{local_dir}{The local directory in which to save files if
`store_locally` is set to `TRUE`. (character)}

\item{store_remotely}{If `TRUE` this function will store the results in an
Amazon Web Services S3 bucket. This will require the user to store an S3
configuration file on their local machine. (logical)}

\item{aws_config_dir}{The local directory in which to save the configuration
file needed for storing data in an AWS S3 bucket. If the file is not yet
present in this directory, the user will be prompted for the information
needed to build the file. (character)}

\item{verbose}{Print verbose output. (logical)}
}
\description{
Retrieves subsetted data of climate future scenarios for National Parks in
the Contiguous United States. This data is downscaled using the Multivariate
Adaptive Constructed Analogs (MACA) technique.
}
\details{
This package retrieves daily gridded data sets of General Climate Models
(GCM) clipped to specified National Parks. Each of these data sets represent
a single GCM, climate variable and Representative Concentration Pathway (RCP)
from 1950 to 2099. The 1950 to 2005 portion of this time period represents
historical data while the 2006 to 2099 portion represents modeled data. These
can be stored as NetCDF files either locally or on an Amazon Web Service S3
bucket. The original data sets may be found at
\url{ http://thredds.northwestknowledge.net:8080/thredds/reacch_climate_
CMIP5_aggregated_macav2_catalog.html}

Production Notes:

- The use of reticulate may enable us to use Zarr arrays, which are
  accessible directly from an s3 bucket.
- This command apparently install python packages automatically through
  conda: "reticulate::conda_install(envname, packages)"
- The R Packages book suggests that we don't put all of these functions into
  file (why not?). It looks like we should split them up and use
  devtools::load_all() instead.
- Where should the default local data storage go? Tmp?
- The builds work find with reticulate, but we will need to take a few more
  steps:
   1) I've not had luck with python 2, though this will more often be the default
      I had to create a .Renviron file with the line:
     
          RETICULATE_PYTHON="/usr/bin/python3"

This won't do, though, its always different

2) In addition to xarray, I needed to install netcdf4, dask, and toolz
   3) We could better ensure that this functions by using a conda environment,
      but that might require extra doing on the users part.
   4) Xarray 0.14 will change the behaviour of open_mfdataset, 0.13 is giving 
      deprecation warnings. Perhaps we pin xarray to 0.12 (same behavior,
      no warnings).
}
